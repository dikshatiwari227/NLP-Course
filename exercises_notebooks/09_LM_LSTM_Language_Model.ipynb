{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_LM_LSTM Language Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMU125G/vGxbFVaCOEc6TqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/09_LM_LSTM_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zwGMzVD0pF"
      },
      "source": [
        "# RNN / BiLSTM and Word Vectors\n",
        "\n",
        "Let's re-use the example you've had with Matthias Lechner, a few weeks back.\n",
        "\n",
        "We'll load the frankenstein book, and convert it into semantic representation through word vectors.\n",
        "\n",
        "We then train a language model, using LSTM, through these vectors.\n",
        "Later, please change the data source from \"frankenstein.txt\" to \"dracula.txt\", and observe the result. \n",
        "\n",
        "### How are we going to do it?\n",
        "\n",
        "We will define our data, as such, that for every word we use as an input for the model (X = Wn), the next word would be the output (Y = Wn+1)\n",
        "\n",
        "The words in the output, Y, will be represented as a one-hot-vector. \n",
        "\n",
        "**Q: What is the size of this Vector?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NTgD9mVh-gi",
        "outputId": "706339b3-51fd-4687-eaf9-da4e5f412c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bpemb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb) (0.1.94)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.6.20)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUhaljjLeco"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import math\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, tensor\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYT3W-Rekse4"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSl2MJaLD-au"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXr-wu8D8P2"
      },
      "source": [
        "Let's convert the text into vectors.\n",
        "\n",
        "We will use a package called [BPEmb](https://nlp.h-its.org/bpemb/) which encodes words to vectors by dividing these words to sub-words, pieces of words, made of characters which often appear together.\n",
        "\n",
        "Q: Remember what is the name of the Linguistic level that deals with letter-level? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKbDV-5qAXc"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWzITC8qCvw",
        "outputId": "7908a4a7-79a3-41cb-9357-a1bc225665f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bpemb_en.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaV2IRzjEClD"
      },
      "source": [
        "Let's create a function to load the corpus data (the books):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVl_hJ8yNktz"
      },
      "source": [
        "def get_file(filename = \"frankenstein.txt\"):\n",
        "  path = tf.keras.utils.get_file(\n",
        "      filename, origin=f\"https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/{filename}\"\n",
        "  )\n",
        "  with open(path, encoding=\"utf-8\") as f:\n",
        "      text = f.read() \n",
        "  text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "  print(\"Corpus length:\", len(text))\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2iZ2Ol3EbU9"
      },
      "source": [
        "# RNN Model\n",
        "And this is the model itself. This is a very raw structure of it. \n",
        "\n",
        "Note: In 'real-ilfe' we're using helping frameworks such as [ignite](https://pytorch.org/ignite/) or [lightning](https://www.pytorchlightning.ai/). \n",
        "\n",
        "We bring it in this version here, for learning purposes only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a768GRY4TXVH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ninp, noutp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "          ninp =  LSTM input size \n",
        "          noutp = size of the output (number of classes)\n",
        "          nhid = number of neurons in the hidden layer\n",
        "          nlayers = number of hidden layer\n",
        "          dropout = dropout rate\n",
        "          tie_weights = whether to use tie_weights (see note)\n",
        "        \"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.noutp = noutp\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding.from_pretrained(tensor(bpemb_en.vectors))\n",
        "        \n",
        "        # self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, noutp)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to ninp (embedding size)')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.noutp)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVkK0WRE4us"
      },
      "source": [
        "A helper class to convert the tokens into batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cc83Hwah0Hc"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFTghmZE8JO"
      },
      "source": [
        "Let's load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJq1xx_Lj813",
        "outputId": "7c56787b-7e4b-47a6-81fb-e7eca3975620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_corpus = get_file('dracula.txt')\n",
        "val_corpus = get_file('frankenstein.txt')\n",
        "\n",
        "print(train_corpus[:300])\n",
        "print(val_corpus[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 842159\n",
            "Corpus length: 420726\n",
            "Dracula, by Bram Stoker  CHAPTER I  JONATHAN HARKER'S JOURNAL  (_Kept in shorthand._)   _3 May. Bistritz._--Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse whic\n",
            "Frankenstein, or, the Modern Prometheus by Mary Wollstonecraft (Godwin) Shelley  Letter 1  _To Mrs. Saville, England._   St. Petersburgh, Dec. 11th, 17—.   You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rA2YyF8E9-5"
      },
      "source": [
        "# Semantic representation + word-parts\n",
        "\n",
        "And convert it into vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4A4ygqiYDd"
      },
      "source": [
        "train_encoded_text = bpemb_en.encode(train_corpus)\n",
        "train_encoded_ids = bpemb_en.encode_ids(train_corpus)\n",
        "\n",
        "val_encoded_text = bpemb_en.encode(val_corpus)\n",
        "val_encoded_ids = bpemb_en.encode_ids(val_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmw96xvFEzL"
      },
      "source": [
        "Let's check the result of encoded_text (we'll get to encoded_ids in a moment).\n",
        "\n",
        "Notice that every word is now broken to pieces. \n",
        "\n",
        "A **'_'** mark in the beginning of a token, represents a beginning of a new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oki7xTl5FDyT",
        "outputId": "468d69aa-c458-43d6-daba-006aa2f6b299",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encoded_text[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁dra',\n",
              " 'c',\n",
              " 'ula',\n",
              " ',',\n",
              " '▁by',\n",
              " '▁br',\n",
              " 'am',\n",
              " '▁st',\n",
              " 'oker',\n",
              " '▁chapter',\n",
              " '▁i',\n",
              " '▁jonathan',\n",
              " '▁har',\n",
              " 'ker',\n",
              " \"'\",\n",
              " 's',\n",
              " '▁journal',\n",
              " '▁(',\n",
              " '_',\n",
              " 'ke',\n",
              " 'pt',\n",
              " '▁in',\n",
              " '▁sh',\n",
              " 'or',\n",
              " 'th',\n",
              " 'and',\n",
              " '.',\n",
              " '_',\n",
              " ')',\n",
              " '▁',\n",
              " '_',\n",
              " '0',\n",
              " '▁may',\n",
              " '.',\n",
              " '▁b',\n",
              " 'ist',\n",
              " 'rit',\n",
              " 'z',\n",
              " '.',\n",
              " '_',\n",
              " '-',\n",
              " '-',\n",
              " 'left',\n",
              " '▁mun',\n",
              " 'ich',\n",
              " '▁at',\n",
              " '▁0:00',\n",
              " '▁p',\n",
              " '.',\n",
              " '▁m']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkpMGwmYFR8E"
      },
      "source": [
        "This method is called word-parts. \n",
        "\n",
        "Instead of converting a whole word (word2vec, gloVe), or a character (FastText), this method converts slices of text, a combination of characters, together.\n",
        "\n",
        "It does so by finding the most common combinations, most frequent combinations, of characters in a very big corpus. \n",
        "\n",
        "The result is having a vocabulary which is WAY smaller than all-the-words (how big would that be?) bug bigger than all the characters:\n",
        "\n",
        "**character-based << word-piece based << word-based**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21-E5O3tF9HK"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGD8BiR7oT11"
      },
      "source": [
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "vocab_size = bpemb_en.vocab_size\n",
        "embsize = bpemb_en.vectors.shape[1]\n",
        "nhidden = 256\n",
        "nlayers = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAdLH-8YuILA"
      },
      "source": [
        "model = RNNModel(embsize, vocab_size, nhidden, nlayers).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2g7-0EkqYZh"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ9zbXUwGBXd"
      },
      "source": [
        "# Division to train/validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyj2AlPwuLui"
      },
      "source": [
        "train_enc_ids = torch.tensor(train_encoded_ids).type(torch.int64)\n",
        "train_data = batchify(train_enc_ids, batch_size)\n",
        "\n",
        "val_enc_ids = torch.tensor(val_encoded_ids).type(torch.int64)\n",
        "val_data = batchify(val_enc_ids, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDDZuY7qyRH"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9TGJMkypM9"
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvF1m90GGtJ"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLQmqGbuq39-"
      },
      "source": [
        "def train(train_data, log_interval = 100):\n",
        "    # Turn on training mode - which enables dropout.\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    start_time = time.time()\n",
        "    ntokens = len(train_data)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        for p in model.parameters():\n",
        "          if p.grad is not None:\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // batch_size, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFFlXWO0MbI"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(data_source)\n",
        "\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, batch_size):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhCcxsTGPk0"
      },
      "source": [
        "# Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd0s9jUuA3O",
        "outputId": "26259a7b-4984-452b-db0b-04380967a2f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  217 batches | lr 20.00 | ms/batch 11.77 | loss  7.22 | ppl  1361.27\n",
            "| epoch   1 |   200/  217 batches | lr 20.00 | ms/batch 10.50 | loss  6.62 | ppl   750.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  2.74s | valid loss  7.04 | valid ppl  1139.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  217 batches | lr 20.00 | ms/batch 10.58 | loss  6.58 | ppl   723.47\n",
            "| epoch   2 |   200/  217 batches | lr 20.00 | ms/batch 10.41 | loss  6.45 | ppl   631.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  2.62s | valid loss  6.97 | valid ppl  1060.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  217 batches | lr 20.00 | ms/batch 10.58 | loss  6.46 | ppl   636.30\n",
            "| epoch   3 |   200/  217 batches | lr 20.00 | ms/batch 10.47 | loss  6.34 | ppl   567.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  2.63s | valid loss  6.90 | valid ppl   990.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  217 batches | lr 20.00 | ms/batch 10.60 | loss  6.34 | ppl   564.17\n",
            "| epoch   4 |   200/  217 batches | lr 20.00 | ms/batch 10.46 | loss  6.20 | ppl   490.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  2.63s | valid loss  6.81 | valid ppl   910.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  217 batches | lr 20.00 | ms/batch 10.60 | loss  6.17 | ppl   475.91\n",
            "| epoch   5 |   200/  217 batches | lr 20.00 | ms/batch 10.48 | loss  6.02 | ppl   412.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  2.64s | valid loss  6.70 | valid ppl   815.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  217 batches | lr 20.00 | ms/batch 10.62 | loss  6.02 | ppl   412.62\n",
            "| epoch   6 |   200/  217 batches | lr 20.00 | ms/batch 10.46 | loss  5.90 | ppl   365.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  2.64s | valid loss  6.64 | valid ppl   768.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  217 batches | lr 20.00 | ms/batch 10.61 | loss  5.90 | ppl   366.44\n",
            "| epoch   7 |   200/  217 batches | lr 20.00 | ms/batch 10.48 | loss  5.80 | ppl   329.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  2.63s | valid loss  6.60 | valid ppl   733.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  217 batches | lr 20.00 | ms/batch 10.72 | loss  5.80 | ppl   331.86\n",
            "| epoch   8 |   200/  217 batches | lr 20.00 | ms/batch 10.49 | loss  5.70 | ppl   297.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  2.65s | valid loss  6.54 | valid ppl   695.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  217 batches | lr 20.00 | ms/batch 10.68 | loss  5.72 | ppl   305.30\n",
            "| epoch   9 |   200/  217 batches | lr 20.00 | ms/batch 10.55 | loss  5.62 | ppl   275.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  2.65s | valid loss  6.49 | valid ppl   657.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  217 batches | lr 20.00 | ms/batch 10.78 | loss  5.65 | ppl   283.78\n",
            "| epoch  10 |   200/  217 batches | lr 20.00 | ms/batch 10.60 | loss  5.55 | ppl   257.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  2.67s | valid loss  6.46 | valid ppl   637.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  217 batches | lr 20.00 | ms/batch 10.73 | loss  5.58 | ppl   265.16\n",
            "| epoch  11 |   200/  217 batches | lr 20.00 | ms/batch 10.66 | loss  5.49 | ppl   242.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  2.67s | valid loss  6.44 | valid ppl   626.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  217 batches | lr 20.00 | ms/batch 10.72 | loss  5.52 | ppl   250.79\n",
            "| epoch  12 |   200/  217 batches | lr 20.00 | ms/batch 10.62 | loss  5.44 | ppl   229.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  2.67s | valid loss  6.42 | valid ppl   616.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  217 batches | lr 20.00 | ms/batch 10.75 | loss  5.47 | ppl   238.02\n",
            "| epoch  13 |   200/  217 batches | lr 20.00 | ms/batch 10.66 | loss  5.39 | ppl   219.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  2.67s | valid loss  6.40 | valid ppl   601.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  217 batches | lr 20.00 | ms/batch 10.71 | loss  5.43 | ppl   227.15\n",
            "| epoch  14 |   200/  217 batches | lr 20.00 | ms/batch 10.63 | loss  5.34 | ppl   209.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  2.67s | valid loss  6.37 | valid ppl   584.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  217 batches | lr 20.00 | ms/batch 10.79 | loss  5.38 | ppl   216.70\n",
            "| epoch  15 |   200/  217 batches | lr 20.00 | ms/batch 10.64 | loss  5.30 | ppl   200.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  2.68s | valid loss  6.37 | valid ppl   583.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  217 batches | lr 20.00 | ms/batch 10.78 | loss  5.34 | ppl   207.61\n",
            "| epoch  16 |   200/  217 batches | lr 20.00 | ms/batch 10.64 | loss  5.26 | ppl   192.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  2.68s | valid loss  6.35 | valid ppl   574.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  217 batches | lr 20.00 | ms/batch 10.83 | loss  5.30 | ppl   200.01\n",
            "| epoch  17 |   200/  217 batches | lr 20.00 | ms/batch 10.71 | loss  5.22 | ppl   185.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  2.70s | valid loss  6.34 | valid ppl   565.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  217 batches | lr 20.00 | ms/batch 10.86 | loss  5.26 | ppl   193.28\n",
            "| epoch  18 |   200/  217 batches | lr 20.00 | ms/batch 10.74 | loss  5.19 | ppl   178.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  2.70s | valid loss  6.32 | valid ppl   553.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  217 batches | lr 20.00 | ms/batch 10.86 | loss  5.22 | ppl   185.38\n",
            "| epoch  19 |   200/  217 batches | lr 20.00 | ms/batch 10.71 | loss  5.15 | ppl   172.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  2.70s | valid loss  6.30 | valid ppl   546.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  217 batches | lr 20.00 | ms/batch 10.90 | loss  5.19 | ppl   179.69\n",
            "| epoch  20 |   200/  217 batches | lr 20.00 | ms/batch 10.72 | loss  5.12 | ppl   167.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  2.70s | valid loss  6.31 | valid ppl   547.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  217 batches | lr 10.00 | ms/batch 10.83 | loss  5.13 | ppl   168.71\n",
            "| epoch  21 |   200/  217 batches | lr 10.00 | ms/batch 10.74 | loss  5.03 | ppl   152.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  2.69s | valid loss  6.29 | valid ppl   538.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  217 batches | lr 10.00 | ms/batch 10.91 | loss  5.09 | ppl   161.96\n",
            "| epoch  22 |   200/  217 batches | lr 10.00 | ms/batch 10.81 | loss  5.00 | ppl   148.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  2.71s | valid loss  6.27 | valid ppl   528.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  217 batches | lr 10.00 | ms/batch 10.78 | loss  5.06 | ppl   158.01\n",
            "| epoch  23 |   200/  217 batches | lr 10.00 | ms/batch 10.75 | loss  4.98 | ppl   145.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  2.70s | valid loss  6.29 | valid ppl   537.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   100/  217 batches | lr 5.00 | ms/batch 10.92 | loss  5.03 | ppl   152.76\n",
            "| epoch  24 |   200/  217 batches | lr 5.00 | ms/batch 10.82 | loss  4.94 | ppl   139.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  2.72s | valid loss  6.27 | valid ppl   530.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   100/  217 batches | lr 2.50 | ms/batch 10.86 | loss  5.00 | ppl   149.06\n",
            "| epoch  25 |   200/  217 batches | lr 2.50 | ms/batch 10.75 | loss  4.90 | ppl   134.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  2.70s | valid loss  6.27 | valid ppl   529.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   100/  217 batches | lr 1.25 | ms/batch 10.94 | loss  4.99 | ppl   146.31\n",
            "| epoch  26 |   200/  217 batches | lr 1.25 | ms/batch 10.82 | loss  4.89 | ppl   133.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  2.72s | valid loss  6.28 | valid ppl   531.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   100/  217 batches | lr 0.62 | ms/batch 10.93 | loss  4.98 | ppl   145.28\n",
            "| epoch  27 |   200/  217 batches | lr 0.62 | ms/batch 10.81 | loss  4.89 | ppl   132.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  2.72s | valid loss  6.28 | valid ppl   533.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   100/  217 batches | lr 0.31 | ms/batch 10.86 | loss  4.97 | ppl   144.64\n",
            "| epoch  28 |   200/  217 batches | lr 0.31 | ms/batch 10.81 | loss  4.88 | ppl   131.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  2.71s | valid loss  6.28 | valid ppl   533.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   100/  217 batches | lr 0.16 | ms/batch 11.00 | loss  4.97 | ppl   143.66\n",
            "| epoch  29 |   200/  217 batches | lr 0.16 | ms/batch 10.81 | loss  4.88 | ppl   131.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  2.73s | valid loss  6.28 | valid ppl   534.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   100/  217 batches | lr 0.08 | ms/batch 10.99 | loss  4.97 | ppl   143.76\n",
            "| epoch  30 |   200/  217 batches | lr 0.08 | ms/batch 10.79 | loss  4.88 | ppl   131.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  2.72s | valid loss  6.28 | valid ppl   534.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   100/  217 batches | lr 0.04 | ms/batch 10.99 | loss  4.97 | ppl   143.84\n",
            "| epoch  31 |   200/  217 batches | lr 0.04 | ms/batch 10.81 | loss  4.88 | ppl   131.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  2.73s | valid loss  6.28 | valid ppl   534.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   100/  217 batches | lr 0.02 | ms/batch 11.02 | loss  4.97 | ppl   144.37\n",
            "| epoch  32 |   200/  217 batches | lr 0.02 | ms/batch 10.83 | loss  4.88 | ppl   131.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  2.73s | valid loss  6.28 | valid ppl   534.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   100/  217 batches | lr 0.01 | ms/batch 10.96 | loss  4.97 | ppl   143.79\n",
            "| epoch  33 |   200/  217 batches | lr 0.01 | ms/batch 10.88 | loss  4.88 | ppl   131.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  2.73s | valid loss  6.28 | valid ppl   534.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   100/  217 batches | lr 0.00 | ms/batch 11.00 | loss  4.97 | ppl   143.99\n",
            "| epoch  34 |   200/  217 batches | lr 0.00 | ms/batch 10.88 | loss  4.88 | ppl   130.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  2.74s | valid loss  6.28 | valid ppl   534.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   100/  217 batches | lr 0.00 | ms/batch 10.95 | loss  4.97 | ppl   144.05\n",
            "| epoch  35 |   200/  217 batches | lr 0.00 | ms/batch 10.83 | loss  4.88 | ppl   131.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  2.73s | valid loss  6.28 | valid ppl   534.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   100/  217 batches | lr 0.00 | ms/batch 11.04 | loss  4.98 | ppl   144.77\n",
            "| epoch  36 |   200/  217 batches | lr 0.00 | ms/batch 10.91 | loss  4.88 | ppl   131.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  2.74s | valid loss  6.28 | valid ppl   534.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   100/  217 batches | lr 0.00 | ms/batch 11.02 | loss  4.97 | ppl   143.68\n",
            "| epoch  37 |   200/  217 batches | lr 0.00 | ms/batch 10.86 | loss  4.87 | ppl   130.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  2.74s | valid loss  6.28 | valid ppl   534.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   100/  217 batches | lr 0.00 | ms/batch 11.07 | loss  4.97 | ppl   143.93\n",
            "| epoch  38 |   200/  217 batches | lr 0.00 | ms/batch 10.83 | loss  4.88 | ppl   131.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  2.74s | valid loss  6.28 | valid ppl   534.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   100/  217 batches | lr 0.00 | ms/batch 11.02 | loss  4.97 | ppl   143.81\n",
            "| epoch  39 |   200/  217 batches | lr 0.00 | ms/batch 10.86 | loss  4.88 | ppl   131.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  2.74s | valid loss  6.28 | valid ppl   534.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   100/  217 batches | lr 0.00 | ms/batch 11.02 | loss  4.97 | ppl   144.08\n",
            "| epoch  40 |   200/  217 batches | lr 0.00 | ms/batch 10.91 | loss  4.88 | ppl   131.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  2.74s | valid loss  6.28 | valid ppl   534.10\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq_PkPQOGVcB"
      },
      "source": [
        "# Text Generation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477FsXfZuDQ3",
        "outputId": "5255f8a4-85c7-4ba3-8dab-1b021fcc9909",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "log_interval = 100\n",
        "words_to_generate = 50\n",
        "temperature = 1. # higher temperature will increase diversity\n",
        "\n",
        "# generate random start\n",
        "input = torch.randint(10000, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "generated_word_ids = []\n",
        "\n",
        "with torch.no_grad():  # no tracking history\n",
        " for i in range(words_to_generate):\n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "\n",
        "    generated_word_ids.append(word_idx.tolist())\n",
        "    # word = bpemb_en.decode_ids([word_idx.tolist()])\n",
        "    # print(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "    # if i % log_interval == 0:\n",
        "    #     print('| Generated {}/{} words'.format(i, words_to_generate))\n",
        "\n",
        "bpemb_en.decode_ids(generated_word_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"air--evenred the old, and sent had not the '. * * * * * _laterame day, morning van helsing._--i have been the count's dataences, but there was somely cle\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9JwCE8GZcD"
      },
      "source": [
        "As discussed in class, the RNN/LSTM can be used to many various task:\n",
        "\n",
        "it can be used for sequence2sequence, where the sequence size is the same or different: \n",
        "* Translation\n",
        "* Tagging words as POS / SLR / NER\n",
        "* Encoding a document as a vector for classification\n",
        "\n",
        "etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeEVfpmQDd3g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}